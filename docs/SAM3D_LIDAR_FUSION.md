# SAM 3D + LiDAR 融合システム 技術仕様書

**作成日:** 2024年12月3日
**目的:** SAM 3Dで生成した3Dオブジェクトを、LiDAR実測データで補正・融合する技術的アプローチの整理

---

## 1. 概要

本仕様書は、以下の2系統のデータを融合して高精度3Dモデルを生成するシステムについて記述する：

- **SAM 3D（生成AI）**: 綺麗なトポロジーを持つ3Dメッシュを生成（裏側も推測で作成）
- **iPad LiDAR（実測）**: 正確な寸法・座標を持つ点群データ

### 1.1 融合のゴール

「SAM 3Dで作った『理想的な形の風船』を、LiDARという『実測の型枠』の中で膨らませて、表面をピタッと吸着させる」イメージ。

---

## 2. SAM 3 と SAM 3D の違い

### 2.1 機能比較

| 項目 | SAM 3 | SAM 3D |
|------|-------|--------|
| **役割** | 「最強のハサミ」（2D切り抜き） | 「3D彫刻家」（立体生成） |
| **入力** | RGB画像 | RGBA画像（背景透明） |
| **出力** | 2Dマスク（バイナリ） | 3Dメッシュ（OBJ/GLB） |
| **3D能力** | なし | あり |
| **裏側生成** | 不可 | 可能（AI推測による） |

### 2.2 処理の流れ

```
1. 元画像（RGB）: iPadで撮影、背景あり
      ↓
2. SAM 3実行: 対象物のマスク生成（白黒画像）
      ↓
3. 切り抜き画像生成: 元画像 × マスク = RGBA画像（背景透明）
      ↓
4. SAM 3D実行: RGBA画像から3Dメッシュ生成（裏側含む）
```

### 2.3 SAM 3Dの強み・弱み

**強み:**
- 構造的に破綻のない綺麗なメッシュ（トポロジー）を生成
- 裏側も推測で作成してくれる

**弱み:**
- 寸法が適当（Unitless）
- 実物と微妙に形が違う可能性（ハルシネーション）

### 2.4 LiDARの強み・弱み

**強み:**
- 寸法（スケール）が絶対的に正しい
- 表面の位置は正確

**弱み:**
- メッシュが汚い、穴あきがある
- 裏側のデータがない（カメラから見える面のみ）

---

## 3. 「裏側」の頂点抽出について

### 3.1 単視点投影の限界

カメラ（RGB）から見えている「オモテ面」の頂点しかマスク情報を持っていない。単純に投影すると：

- 裏側は選択されない
- 最悪の場合、「パンチスルー問題」（物体を貫通して裏の壁・床まで誤選択）

### 3.2 解決策

#### A. マルチビュー統合（必須）

iPadで対象の周りをぐるっと回りながら撮影し、複数視点からのマスクを3D空間上で統合（Vote/Fusion）する。

#### B. 領域拡張アルゴリズム

オモテ面のマスクをシード（種）として、3Dメッシュ上の接続性（グラフカットやリージョングローイング）を利用して裏側まで選択範囲を広げる。

---

## 4. 2系統データの融合ワークフロー

この融合処理は単純な「足し算」ではなく、数学的な「変形（Deformation）」処理。

### 4.1 Step 1: 大まかな位置合わせ（Coarse Alignment）

SAM 3Dのモデルを、LiDAR点群の座標系に合わせる。

**処理:**
- SAM 3Dメッシュの重心をLiDAR点群の中心に移動
- バウンディングボックスのサイズ比率でスケーリング

**技術:**
- PCA（主成分分析）による主軸合わせ
- Global Registrationアルゴリズム

### 4.2 Step 2: 精密な位置合わせ（Rigid ICP）

形を変えずに、位置と回転だけを微調整。

**技術:** ICP (Iterative Closest Point) アルゴリズム

**結果:** SAM 3Dのモデルが、LiDAR点群の中に「埋まる」位置に配置される

### 4.3 Step 3: メッシュの変形・吸着（Non-rigid Registration / Shrinkwrap）★核心

SAM 3Dの頂点を動かして、LiDARの点群に吸着させる。

**Blenderでの実装:**
```
Shrinkwrap Modifier
├── ターゲット: iPad LiDARメッシュ
├── ソース: SAM 3Dの生成メッシュ
└── 設定: 頂点をターゲット表面に投影して移動
```

**Python (Open3D) での実装:**
- Non-rigid ICP を使用
- メッシュの滑らかさを維持する制約（正則化項）をかけつつ変形

---

## 5. 融合時の課題と解決策

### 5.1 トポロジーの不一致問題

SAM 3Dが「4本脚の椅子」を生成し、実物が「キャスター付きの椅子」だった場合、無理やり融合させるとメッシュが破綻。

**解決策:**
- 融合前にCLIPモデル等で「形状の類似度」を判定
- 乖離が大きい場合はSAM 3Dに再生成させるか、融合をスキップ

### 5.2 「裏側」の扱い（重要）

LiDARデータは「オモテ面」しかない。単純にShrinkwrapさせると、SAM 3Dの「裏側」メッシュまで無理やり引っ張られて潰れるリスク。

**解決策:**
- **可視判定（Visibility Check）**: カメラ位置から見て「見えている頂点」だけをLiDARデータに吸着
- 「見えていない裏側」はSAM 3Dの形状をそのまま維持
- BlenderのVertex Group機能で制御可能

---

## 6. 点群抽出の実現可能性

### 6.1 結論

「3Dオブジェクトに該当する点群情報を抽出して」は**実現可能**。標準的なエンジニアリングの範疇。

### 6.2 抽出ロジック（逆転の発想）

「3D点群の中から椅子を探す」のではなく、「3D点群を全部2D画面に投影してみて、SAM 3のマスクの中に落ちた点だけを拾う」

```python
# 処理の流れ
1. 入力: 部屋全体の点群（数万点）+ SAM 3のマスク画像
2. 投影: すべての3D点をカメラ位置から見た2D座標に変換
   - 2D座標 = カメラ内部行列 × カメラ外部行列 × 3D座標
3. 判定:
   - 点Aの投影先 → ピクセル(100, 200) → マスク上で「黒」 → 捨てる
   - 点Bの投影先 → ピクセル(350, 400) → マスク上で「白」 → 抽出
4. 結果: マスク範囲内の3D点群だけが残る
```

この計算はPython（NumPy, Open3D）で数万点あっても数ミリ秒で完了。

### 6.3 実用上の落とし穴

#### A. パンチスルー問題（突き抜け）

2Dマスクだけで判定すると、「椅子の手前のゴミ」も「椅子の後ろの壁」も選択されてしまう。

**対策:**
- 深度（Depth）情報でフィルタリング（例: 1m〜2mの範囲のみ）
- DBSCAN（密度ベースクラスタリング）で塊を分離

#### B. 位置ズレ（Synchronization）

RGB画像とLiDAR点群の取得タイミングのズレでマスクと点群が微妙にずれる。

**対策:**
- マスクを数ピクセル分「太らせて（Dilation）」余裕を持たせる
- あとで外れ値を捨てる

---

## 7. LLMによるパイプライン指揮

### 7.1 LLMの役割

LLMは3D処理の計算自体を行うのではなく、「各処理プロセスを適切な順番・パラメータで呼び出し、結果を判断して次の指示を出す司令塔（オーケストレーター）」として機能。

### 7.2 ツール定義

LLMに以下の「道具（Tools / Functions）」を与える：

```python
run_sam3d(image_path)  # SAM 3Dを実行
run_icp_alignment(source_mesh, target_pcd)  # ICP位置合わせ
run_blender_fusion(base_mesh, target_pcd, visibility_threshold)  # Blender吸着
```

### 7.3 処理フローの例

```
ステップ1: 生成 (SAM 3D)
  LLM: 「まずはRGB画像からベースモデルを作ります」
  Action: run_sam3d("input.jpg")
  Result: Success: Created "sam_mesh.obj"

ステップ2: 位置合わせ (ICP)
  LLM: 「次に、LiDAR点群と位置合わせをします」
  Action: run_icp_alignment("sam_mesh.obj", "lidar.ply")
  Result: Alignment Score = 0.4 (Low)
  LLM (判断): 「スコア0.4は低すぎます。初期回転を90度ずらして再試行」
  Action (再試行): run_icp_alignment(..., init_rotation=90)
  Result: Alignment Score = 0.95 (High)

ステップ3: 補正 (Blender Fusion)
  LLM: 「Blenderを起動して、可視領域のみを吸着させます」
  Action: run_blender_fusion(..., method="shrinkwrap")
  Result: Success: Output saved to "final_model.usdz"
```

### 7.4 LLMを使うメリット

1. **エラーハンドリングの自動化**: ICPが失敗したらパラメータ変更して再実行
2. **曖昧な指示の反映**: 「もっと滑らかにして」→ smooth_iterations を調整
3. **MCP (Model Context Protocol) との親和性**: DGXのツールをリモートから呼び出し可能

---

## 8. 段階的開発計画

### 8.1 Step 1: 一本道のスクリプト（今すぐ着手可能）

エラー処理を考えず、正常系で `SAM 3D → ICP → Blender` が繋がるPythonコードを完成させる。

**目的:** 技術検証

### 8.2 Step 2: LLMエージェントで包む（デモ品質向上）

Step 1で作った関数を「ツール」としてLLM（Claude）に渡し、以下のようなプロンプトで制御：

> 「もしICPのスコアが0.5以下なら、パラメータを変えて再実行せよ」

**目的:** ロバスト性向上、ユーザー対話

---

## 9. 推奨システム構成

### 9.1 サーバーサイド（DGX）の実装ステップ

```
1. 入力: iPadから「LiDAR点群」と「RGB画像」を受け取る

2. 生成: RGB画像をSAM 3Dに入力 → Generated_Mesh.obj を生成

3. 位置合わせ: Generated_Mesh をLiDAR点群にICPで位置合わせ

4. 補正 (Fusion): Blenderをバックグラウンド起動
   - Generated_Mesh のうち、カメラから見える範囲の頂点を特定
   - その頂点だけをLiDAR点群に Shrinkwrap（吸着）させる
   - 裏側の形状は維持される

5. 出力: 実測値に即しつつ、裏側も破綻していない綺麗な3Dモデルが完成
```

### 9.2 推奨ライブラリ構成

| 用途 | ライブラリ |
|------|-----------|
| 点群処理 | Open3D |
| 行列計算 | NumPy |
| メッシュ変形 | PyTorch3D / Blender API |
| LLMエージェント | LangGraph / AutoGen |
| LLM | Claude 3.5 Sonnet（コーディング能力が高い） |

---

## 10. まとめ

### 10.1 技術的妥当性

SAM 3Dで綺麗なベースモデルを作り、LiDARデータで寸法と表面形状を矯正するパイプラインは：

- **極めて妥当性が高い**
- 現在の3D再構成技術のトレンドと合致
- Template Fitting / Non-rigid Registration という確立された技術領域

### 10.2 キーポイント

1. **SAM 3 → SAM 3D の連携**: マスクではなく「背景透明のカラー画像」を渡す
2. **可視判定による部分的吸着**: 裏側を潰さない
3. **LLMはオーケストレーター**: 座標処理はさせない、ツールを指揮させる

### 10.3 次のアクション

1. 各ツール（SAM 3D, ICP, Blender）の連携確認スクリプトを作成
2. 可視判定による部分的吸着を実装
3. LLMエージェント化は技術検証後
